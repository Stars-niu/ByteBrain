import streamlit as st
from modelscope import snapshot_download
from typing import List, Dict
import numpy as np
import torch
from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling
import os
import json
from datasets import load_dataset, Dataset

# è®¾ç½® CUDA ç¯å¢ƒå˜é‡
os.environ["CUDA_LAUNCH_BLOCKING"] = "1"

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="ByteBrain",
    page_icon="ğŸª",
    layout="wide",
    initial_sidebar_state="auto",
    menu_items={
        "Get Help": "https://bytebrain.com/help",
        "Report a bug": "https://bytebrain.com/bug-report",
        "About": "https://bytebrain.com/about"
    }
)

# æ·»åŠ è‡ªå®šä¹‰ CSS æ ·å¼ï¼Œç¾åŒ–é¡µé¢
st.markdown(
    """
    <style>
    body {
        background-color: #282c34; /* æ·±ç°è‰²èƒŒæ™¯ */
        color: #fff; /* ç™½è‰²æ–‡å­— */
        font-family: Arial, sans-serif; /* å­—ä½“ */
    }
.main {
        padding: 20px;
    }
.stButton>button {
        background-color: #61dafb; /* æµ…è“è‰²æŒ‰é’®èƒŒæ™¯ */
        color: #000; /* é»‘è‰²æ–‡å­— */
        border: none;
        padding: 10px 24px;
        text-align: center;
        text-decoration: none;
        display: inline-block;
        font-size: 16px;
        margin: 4px 2px;
        cursor: pointer;
        border-radius: 12px;
    }
.stTextInput>div>div>input {
        border: 2px solid #61dafb; /* æµ…è“è‰²è¾¹æ¡† */
        border-radius: 12px;
        padding: 10px;
        background-color: #fff; /* ç™½è‰²è¾“å…¥æ¡†èƒŒæ™¯ */
        color: #000; /* é»‘è‰²æ–‡å­— */
    }
.fixed-right {
        position: fixed;
        top: 20px;
        right: 20px;
        width: 30%;
    }
.chat-container {
        width: 100%;
        max-height: 70vh;
        overflow-y: auto;
        padding-right: 20px;
    }
.stChatMessage {
        width: 100%;
    }
.answer-box {
        background-color: rgba(0, 0, 0, 0.5);
        padding: 15px;
        border-radius: 12px;
        margin-top: 20px;
    }
    h1 {
        color: #61dafb; /* æµ…è“è‰²æ ‡é¢˜ */
    }
    h2 {
        color: #98c379; /* æµ…ç»¿è‰²å‰¯æ ‡é¢˜ */
    }
    </style>
    """,
    unsafe_allow_html=True,
)

# åˆ›å»ºä¸€ä¸ªæ ‡é¢˜å’Œä¸€ä¸ªå‰¯æ ‡é¢˜
st.markdown(
    '<h1>âœ¨ ByteBrain</h1><h2>â€”â€”è®¡ç®—æœºç§‘å­¦æ™ºèƒ½çŸ¥è¯†åŠ©æ‰‹</h2>',
    unsafe_allow_html=True
)

# å‘é‡æ¨¡å‹ä¸‹è½½
try:
    embed_model_dir = snapshot_download("AI-ModelScope/bge-small-zh-v1.5", cache_dir='.')
    print(f'Successfully loaded embedding model from cache.')
except Exception as e:
    st.error(f'Error loading embedding model: {e}')
    embed_model_dir = None

# æºå¤§æ¨¡å‹ä¸‹è½½
try:
    llm_model_dir = snapshot_download('IEITYuan/Yuan2-2B-Mars-hf', cache_dir='.')
    print(f'Successfully loaded LLM model from cache.')
except Exception as e:
    st.error(f'Error loading LLM model: {e}')
    llm_model_dir = None

# å®šä¹‰å‘é‡æ¨¡å‹ç±»
class EmbeddingModel:
    """
    class for EmbeddingModel
    """

    def __init__(self, path: str) -> None:
        try:
            # åŠ è½½é¢„è®­ç»ƒçš„åˆ†è¯å™¨å’Œæ¨¡å‹
            self.tokenizer = AutoTokenizer.from_pretrained(path)
            self.model = AutoModel.from_pretrained(path)
            if torch.cuda.is_available():
                self.model = self.model.cuda()
            print(f'Loading EmbeddingModel from {path}.')
        except Exception as e:
            print(f'Error initializing EmbeddingModel: {e}')

    def get_embeddings(self, texts: List[str]) -> List[float]:
        """
        calculate embedding for text list
        """
        try:
            # å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œç¼–ç 
            encoded_input = self.tokenizer(texts, padding=True, truncation=True, return_tensors='pt')
            if torch.cuda.is_available():
                encoded_input = {k: v.cuda() for k, v in encoded_input.items()}
            with torch.no_grad():
                # è·å–æ¨¡å‹è¾“å‡º
                model_output = self.model(**encoded_input)
                sentence_embeddings = model_output[0][:, 0]
            # å¯¹åµŒå…¥å‘é‡è¿›è¡Œå½’ä¸€åŒ–
            sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)
            return sentence_embeddings.tolist()
        except Exception as e:
            print(f'Error in get_embeddings: {e}')
            return []

print("> Create embedding model...")
if embed_model_dir:
    embed_model = EmbeddingModel(embed_model_dir)
else:
    st.error('Embedding model not loaded. Cannot continue.')
    embed_model = None

# å®šä¹‰å‘é‡åº“ç´¢å¼•ç±»
class VectorStoreIndex:
    """
    class for VectorStoreIndex
    """

    def __init__(self, document_path: str, embed_model: EmbeddingModel) -> None:
        try:
            # åŠ è½½æ–‡æ¡£
            self.documents = []
            for line in open(document_path, 'r', encoding='utf-8'):
                line = line.strip()
                self.documents.append(line)

            self.embed_model = embed_model
            # è·å–æ–‡æ¡£çš„åµŒå…¥å‘é‡
            self.vectors = self.embed_model.get_embeddings(self.documents)

            print(f'Loading {len(self.documents)} documents for {document_path}.')
        except Exception as e:
            print(f'Error initializing VectorStoreIndex: {e}')

    def get_similarity(self, vector1: List[float], vector2: List[float]) -> float:
        """
        calculate cosine similarity between two vectors
        """
        try:
            dot_product = np.dot(vector1, vector2)
            magnitude = np.linalg.norm(vector1) * np.linalg.norm(vector2)
            if not magnitude:
                return 0
            return dot_product / magnitude
        except Exception as e:
            print(f'Error in get_similarity: {e}')
            return 0

    def query(self, question: str, k: int = 1) -> List[str]:
        """
        query the vector store for similar documents
        """
        try:
            # è·å–é—®é¢˜çš„åµŒå…¥å‘é‡
            question_vector = self.embed_model.get_embeddings([question])[0]
            # è®¡ç®—é—®é¢˜å‘é‡ä¸æ–‡æ¡£å‘é‡çš„ç›¸ä¼¼åº¦
            result = np.array([self.get_similarity(question_vector, vector) for vector in self.vectors])
            # è¿”å›ç›¸ä¼¼åº¦æœ€é«˜çš„æ–‡æ¡£
            return np.array(self.documents)[result.argsort()[-k:][::-1]].tolist()
        except Exception as e:
            print(f'Error in query: {e}')
            return []

print("> Create index...")
document_path = './knowledge.txt'
if embed_model:
    index = VectorStoreIndex(document_path, embed_model)
else:
    st.error('Embedding model not available. Cannot create index.')
    index = None

# å®šä¹‰å¤§è¯­è¨€æ¨¡å‹ç±»ï¼ˆåŒ…å«å¾®è°ƒåŠŸèƒ½ï¼‰
class LLM:
    """
    class for Yuan2.0 LLM
    """

    def __init__(self, model_path: str) -> None:
        try:
            print("Create tokenizer...")
            # åŠ è½½é¢„è®­ç»ƒçš„åˆ†è¯å™¨
            self.tokenizer = AutoTokenizer.from_pretrained(model_path, add_eos_token=False, add_bos_token=False, eos_token='<eod>', legacy=False)
            self.tokenizer.add_tokens(['<sep>', '<pad>', '<mask>', '<predict>', '<FIM_SUFFIX>', '<FIM_PREFIX>', '<FIM_MIDDLE>','<commit_before>','<commit_msg>','<commit_after>','<jupyter_start>','<jupyter_text>','<jupyter_code>','<jupyter_output>','<empty_output>'], special_tokens=True)

            print("Create model...")
            # åŠ è½½é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹
            self.model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16, trust_remote_code=True)
            if torch.cuda.is_available():
                self.model = self.model.cuda()

            print(f'Loading Yuan2.0 model from {model_path}.')
        except Exception as e:
            print(f'Error initializing LLM: {e}')

    def generate(self, question: str, context: List[str]) -> str:
        """
        generate answer using the language model
        """
        try:
            # æ„å»ºæç¤ºè¯
            if context:
                prompt = f'èƒŒæ™¯ï¼š{context}\né—®é¢˜ï¼š{question}\nè¯·åŸºäºèƒŒæ™¯ï¼Œå›ç­”é—®é¢˜ã€‚'
            else:
                prompt = question

            prompt += "<sep>"
            inputs = self.tokenizer(prompt, return_tensors="pt")["input_ids"]
            if torch.cuda.is_available():
                inputs = inputs.cuda()

            # æˆªæ–­è¾“å…¥ä»¥ç¡®ä¿é•¿åº¦ä¸è¶…è¿‡æœ€å¤§é•¿åº¦
            max_length = 2048
            if inputs.shape[1] > max_length:
                inputs = inputs[:, -max_length:]

            # ç¡®ä¿è¾“å…¥æ•°æ®ç±»å‹æ­£ç¡®
            if inputs.dtype == torch.bfloat16:
                inputs = inputs.to(torch.long)
            elif inputs.dtype!= torch.long:
                st.error(f'Unexpected input data type: {inputs.dtype}. Expected torch.long.')
                return 'Sorry, an error occurred while generating the answer.'

            # ç”Ÿæˆè¾“å‡ºæ—¶ç¡®ä¿æ¨¡å‹å’Œè¾“å…¥æ•°æ®çš„æ•°æ®ç±»å‹ä¸€è‡´
            if torch.cuda.is_available():
                self.model.to(torch.bfloat16)
            inputs = inputs.to(torch.bfloat16)

            outputs = self.model.generate(inputs, do_sample=False, max_new_tokens=512)
            output = self.tokenizer.decode(outputs[0])

            # ç§»é™¤ä¸éœ€è¦çš„å­—ç¬¦
            output = output.split("<sep>")[-1].replace("<eod>", "").strip()

            return output
        except Exception as e:
            print(f'Error in generate: {e}')
            return 'Sorry, an error occurred while generating the answer.'

    def fine_tune(self, data_path: str) -> None:
        """
        fine-tune the language model
        """
        try:
            # åŠ è½½å¾®è°ƒæ•°æ®
            dataset = load_dataset('json', data_files=data_path, split='train')

            def preprocess_function(examples):
                inputs = examples['input_text']
                targets = examples['output_text']
                model_inputs = self.tokenizer(inputs, max_length=512, truncation=True)

                # Setup the tokenizer for targets
                with self.tokenizer.as_target_tokenizer():
                    labels = self.tokenizer(targets, max_length=512, truncation=True)

                model_inputs['labels'] = labels['input_ids']
                return model_inputs

            tokenized_dataset = dataset.map(preprocess_function, batched=True)
            data_collator = DataCollatorForLanguageModeling(tokenizer=self.tokenizer, mlm=False)

            training_args = TrainingArguments(
                output_dir='./results',
                overwrite_output_dir=True,
                num_train_epochs=3,
                per_device_train_batch_size=4,
                save_steps=10_000,
                save_total_limit=2,
                logging_dir='./logs',
                logging_steps=200,
            )

            trainer = Trainer(
                model=self.model,
                args=training_args,
                data_collator=data_collator,
                train_dataset=tokenized_dataset,
            )

            trainer.train()
        except Exception as e:
            print(f'Error in fine_tune: {e}')

print("> Create LLM model...")
if llm_model_dir:
    llm_model = LLM(llm_model_dir)
else:
    st.error('LLM model not loaded. Cannot continue.')
    llm_model = None

# Streamlit ç”¨æˆ·ç•Œé¢é€»è¾‘
st.sidebar.title("ByteBrain ä¾§è¾¹æ ")
user_input = st.sidebar.text_area("è¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼š", "")

if st.sidebar.button("æŸ¥è¯¢"):
    if user_input:
        st.sidebar.markdown("### æŸ¥è¯¢ç»“æœ")

# Streamlit ç”¨æˆ·ç•Œé¢é€»è¾‘
st.sidebar.title("ByteBrain ä¾§è¾¹æ ")
user_input = st.sidebar.text_area("è¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼š", "")

if st.sidebar.button("æŸ¥è¯¢"):
    if user_input:
        st.sidebar.markdown("### æŸ¥è¯¢ç»“æœ")
        # æŸ¥è¯¢ VectorStoreIndex
        try:
            context = index.query(user_input)
            st.sidebar.markdown("#### ä¸Šä¸‹æ–‡")
            for doc in context:
                st.sidebar.markdown(f"- {doc}")
        except Exception as e:
            st.sidebar.error(f'æŸ¥è¯¢æ—¶å‡ºç°é”™è¯¯ï¼š{e}')
        
        # ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆå›ç­”
        if llm_model:
            try:
                answer = llm_model.generate(user_input, context)
                st.sidebar.markdown("#### å›ç­”")
                st.sidebar.markdown(f"{answer}")
            except Exception as e:
                st.sidebar.error(f'ç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯ï¼š{e}')
        else:
            st.sidebar.error("è¯­è¨€æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆå›ç­”ã€‚")
    else:
        st.sidebar.warning("è¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼")

# ä¸»é¡µé¢
st.title("ByteBrain")
st.write("æ¬¢è¿ä½¿ç”¨ ByteBrainï¼Œä¸€ä¸ªè®¡ç®—æœºç§‘å­¦æ™ºèƒ½çŸ¥è¯†åŠ©æ‰‹ï¼è¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼Œè·å–ä¸“ä¸šçš„å›ç­”ã€‚")
