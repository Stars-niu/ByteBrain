# å¯¼å…¥æ‰€éœ€çš„åº“
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import streamlit as st
from modelscope import snapshot_download
from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer, DPRContextEncoder, DPRContextEncoderTokenizer
import fitz  # PyMuPDF
import docx
import faiss
import numpy as np

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="ByteBrain",
    page_icon="ğŸª",
    layout="wide",
    initial_sidebar_state="auto",
    menu_items={
        "Get Help": "https://bytebrain.com/help",
        "Report a bug": "https://bytebrain.com/bug-report",
        "About": "https://bytebrain.com/about"
    }
)

# æ·»åŠ è‡ªå®šä¹‰CSSæ ·å¼
st.markdown(
    """
    <style>
    .main {
        background-image: url('Background.png');
        background-size: cover;
        background-position: center;
        padding: 10px;
    }
    .stButton>button {
        background-color: #4CAF50;
        color: blue;
        border: none;
        padding: 10px 24px;
        text-align: center;
        text-decoration: none;
        display: inline-block;
        font-size: 16px;
        margin: 4px 2px;
        cursor: pointer;
        border-radius: 12px;
    }
    .stTextInput>div>div>input {
        border: 2px solid #4CAF50;
        border-radius: 12px;
        padding: 10px;
    }
    .fixed-right {
        position: fixed;
        top: 20px;
        right: 20px;
        width: 30%;
    }
    .chat-container {
        width: 100%;
        max-height: 70vh;
        overflow-y: auto;
        padding-right: 20px;
    }
    .stChatMessage {
        width: 100%;
    }
    </style>
    """,
    unsafe_allow_html=True,
)

st.markdown(
    '<span style="font-size: 60px">âœ¨ ByteBrain</span>&nbsp;&nbsp;<span style="font-size: 24px">â€”â€”è®¡ç®—æœºç§‘å­¦æ™ºèƒ½çŸ¥è¯†åŠ©æ‰‹</span>',
    unsafe_allow_html=True
)

# æºå¤§æ¨¡å‹ä¸‹è½½
model_dir = snapshot_download('IEITYuan/Yuan2-2B-July-hf', cache_dir='./')

# å®šä¹‰æ¨¡å‹è·¯å¾„
path = './IEITYuan/Yuan2-2B-July-hf'

# å®šä¹‰æ¨¡å‹æ•°æ®ç±»å‹
torch_dtype = torch.bfloat16 # A10

# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºè·å–æ¨¡å‹å’Œtokenizer
@st.cache_resource
def get_model():
    print("åˆ›å»ºtokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(path, add_eos_token=False, add_bos_token=False, eos_token='<eod>')
    tokenizer.add_tokens(['<sep>', '<pad>', '<mask>', '<predict>', '<FIM_SUFFIX>', '<FIM_PREFIX>', '<FIM_MIDDLE>','<commit_before>','<commit_msg>','<commit_after>','<jupyter_start>','<jupyter_text>','<jupyter_code>','<jupyter_output>','<empty_output>'], special_tokens=True)

    print("åˆ›å»ºæ¨¡å‹...")
    model = AutoModelForCausalLM.from_pretrained(path, torch_dtype=torch_dtype, trust_remote_code=True).cuda()

    print("å®Œæˆ.")
    return tokenizer, model

# åŠ è½½modelå’Œtokenizer
tokenizer, model = get_model()

# åŠ è½½DPRæ¨¡å‹å’Œtokenizer
@st.cache_resource
def get_dpr_models():
    question_encoder = DPRQuestionEncoder.from_pretrained("facebook/dpr-question_encoder-single-nq").cuda()
    question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained("facebook/dpr-question_encoder-single-nq")
    context_encoder = DPRContextEncoder.from_pretrained("facebook/dpr-ctx_encoder-single-nq").cuda()
    context_tokenizer = DPRContextEncoderTokenizer.from_pretrained("facebook/dpr-ctx_encoder-single-nq")
    return question_encoder, question_tokenizer, context_encoder, context_tokenizer

question_encoder, question_tokenizer, context_encoder, context_tokenizer = get_dpr_models()

# è¯»å–PDFæ–‡ä»¶
def read_pdf(file):
    doc = fitz.open(stream=file.read(), filetype="pdf")
    text = ""
    for page in doc:
        text += page.get_text()
    return text

# è¯»å–Wordæ–‡ä»¶
def read_word(file):
    doc = docx.Document(file)
    text = ""
    for para in doc.paragraphs:
        text += para.text + "\n"
    return text

# åŠ è½½çŸ¥è¯†åº“
@st.cache_resource
def load_knowledge_base():
    return []

knowledge_base = load_knowledge_base()

# æ„å»ºå‘é‡æ•°æ®åº“
@st.cache_resource
def build_faiss_index(knowledge_base):
    context_embeddings = []
    for doc in knowledge_base:
        inputs = context_tokenizer(doc, return_tensors="pt", truncation=True, padding=True).to("cuda")
        embedding = context_encoder(**inputs).pooler_output.cpu().detach().numpy()
        context_embeddings.append(embedding)
    context_embeddings = np.vstack(context_embeddings)
    index = faiss.IndexFlatL2(context_embeddings.shape[1])
    index.add(context_embeddings)
    return index

index = build_faiss_index(knowledge_base)

# åˆæ¬¡è¿è¡Œæ—¶ï¼Œsession_stateä¸­æ²¡æœ‰"messages"ï¼Œéœ€è¦åˆ›å»ºä¸€ä¸ªç©ºåˆ—è¡¨
if "messages" not in st.session_state:
    st.session_state["messages"] = []

# ä½¿ç”¨åˆ†æ å¸ƒå±€
col1, col2 = st.columns([2, 1])

with col1:
    st.markdown("<div class='chat-container'>", unsafe_allow_html=True)
    # æ¯æ¬¡å¯¹è¯æ—¶ï¼Œéƒ½éœ€è¦éå†session_stateä¸­çš„æ‰€æœ‰æ¶ˆæ¯ï¼Œå¹¶æ˜¾ç¤ºåœ¨èŠå¤©ç•Œé¢ä¸Š
    for msg in st.session_state.messages:
        st.chat_message(msg["role"]).write(msg["content"])
    st.markdown("</div>", unsafe_allow_html=True)

with col2:
    st.markdown("<div class='fixed-right'>", unsafe_allow_html=True)
    st.image("logo.png", caption="ByteBrain Logo", width=150)
    st.markdown("ByteBrainâ€”â€”ä¸€ä¸ªæ™ºèƒ½çŸ¥è¯†åŠ©æ‰‹ï¼Œæ—¨åœ¨å¸®åŠ©ç”¨æˆ·å¿«é€Ÿè·å–ä¿¡æ¯å’Œè§£å†³é—®é¢˜ã€‚")
    st.markdown("### è”ç³»æˆ‘ä»¬")
    st.markdown("å¦‚æœæ‚¨æœ‰ä»»ä½•é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼è”ç³»æˆ‘ä»¬ï¼š")
    st.markdown("- é‚®ç®±: support@bytebrain.com")
    st.markdown("- ç”µè¯: 520-1314")
    st.markdown("</div>", unsafe_allow_html=True)

# æ–‡ä»¶ä¸Šä¼ 
uploaded_file = st.file_uploader("ä¸Šä¼ PDFæˆ–Wordæ–‡ä»¶", type=["pdf", "docx"])

if uploaded_file:
    if uploaded_file.type == "application/pdf":
        file_text = read_pdf(uploaded_file)
    elif uploaded_file.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
        file_text = read_word(uploaded_file)
    
    # å°†æ–‡ä»¶å†…å®¹æ·»åŠ åˆ°çŸ¥è¯†åº“ä¸­
    knowledge_base.append(file_text)
    
    # é‡æ–°æ„å»ºå‘é‡æ•°æ®åº“
    index = build_faiss_index(knowledge_base)

# èŠå¤©è¾“å…¥æ¡†æ”¾åœ¨col1ä¹‹å¤–
prompt = st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜:")

if prompt:
    # å°†ç”¨æˆ·çš„è¾“å…¥æ·»åŠ åˆ°session_stateä¸­çš„messagesåˆ—è¡¨ä¸­
    st.session_state.messages.append({"role": "user", "content": prompt})

    # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºç”¨æˆ·çš„è¾“å…¥
    st.chat_message("user").write(prompt)

    # ä½¿ç”¨DPRæ¨¡å‹è¿›è¡Œæ£€ç´¢
    inputs = question_tokenizer(prompt, return_tensors="pt", truncation=True, padding=True).to("cuda")
    question_embedding = question_encoder(**inputs).pooler_output.cpu().detach().numpy()
    D, I = index.search(question_embedding, k=5)  # æ£€ç´¢å‰5ä¸ªç›¸å…³æ–‡æ¡£
    retrieved_docs = [knowledge_base[i] for i in I[0]]

    # å°†æ£€ç´¢åˆ°çš„æ–‡æ¡£æ‹¼æ¥åˆ°promptä¸­
    prompt_with_docs = prompt + "\n\n" + "\n\n".join(retrieved_docs) + "<sep>"

    # è°ƒç”¨ç”Ÿæˆæ¨¡å‹
    inputs = tokenizer(prompt_with_docs, return_tensors="pt")["input_ids"].cuda()
    outputs = model.generate(inputs, do_sample=False, max_length=1024)  # è®¾ç½®è§£ç æ–¹å¼å’Œæœ€å¤§ç”Ÿæˆé•¿åº¦
    output = tokenizer.decode(outputs[0])
    response = output.split("<sep>")[-1].replace("<eod>", '')

    # å°†æ¨¡å‹çš„è¾“å‡ºæ·»åŠ åˆ°session_stateä¸­çš„messagesåˆ—è¡¨ä¸­
    st.session_state.messages.append({"role": "assistant", "content": response})

    # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºæ¨¡å‹çš„è¾“å‡º
    st.chat_message("assistant").write(response)
# ä»£ç è¯´æ˜
# è¯»å–PDFå’ŒWordæ–‡ä»¶ï¼šä½¿ç”¨fitzåº“è¯»å–PDFæ–‡ä»¶ï¼Œä½¿ç”¨docxåº“è¯»å–Wordæ–‡ä»¶ã€‚
# æ–‡ä»¶ä¸Šä¼ åŠŸèƒ½ï¼šä½¿ç”¨Streamlitçš„file_uploaderç»„ä»¶å…è®¸ç”¨æˆ·ä¸Šä¼ PDFæˆ–Wordæ–‡ä»¶ã€‚
# å°†æ–‡ä»¶å†…å®¹æ·»åŠ åˆ°çŸ¥è¯†åº“ï¼šè¯»å–æ–‡ä»¶å†…å®¹å¹¶æ·»åŠ åˆ°çŸ¥è¯†åº“ä¸­ã€‚
# é‡æ–°æ„å»ºå‘é‡æ•°æ®åº“ï¼šæ¯æ¬¡æ·»åŠ æ–°æ–‡ä»¶åï¼Œé‡æ–°æ„å»ºå‘é‡æ•°æ®åº“ã€‚

# å¯ä»¥ä½¿ç”¨ä¸€äº›Pythonåº“æ¥å¤„ç†PDFå’ŒWordæ–‡æ¡£ï¼Œä¾‹å¦‚PyMuPDFï¼ˆåˆåfitzï¼‰ç”¨äºPDFï¼Œpython-docxç”¨äºWordæ–‡æ¡£ã€‚

# ä¸‹é¢æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼Œå±•ç¤ºäº†å¦‚ä½•è¯»å–PDFå’ŒWordæ–‡æ¡£å¹¶å°†å…¶å†…å®¹æ·»åŠ åˆ°çŸ¥è¯†åº“ä¸­ï¼š

# å®‰è£…æ‰€éœ€çš„åº“
# bash
# pip install PyMuPDF python-docx
# è¯»å–PDFå’ŒWordæ–‡æ¡£çš„ç¤ºä¾‹ä»£ç 
# python
# import fitz  # PyMuPDF
# import docx

# def read_pdf(file_path):
#     """è¯»å–PDFæ–‡ä»¶å¹¶è¿”å›æ–‡æœ¬å†…å®¹"""
#     doc = fitz.open(file_path)
#     text = ""
#     for page in doc:
#         text += page.get_text()
#     return text

# def read_word(file_path):
#     """è¯»å–Wordæ–‡ä»¶å¹¶è¿”å›æ–‡æœ¬å†…å®¹"""
#     doc = docx.Document(file_path)
#     text = ""
#     for para in doc.paragraphs:
#         text += para.text + "\n"
#     return text

# # ç¤ºä¾‹ï¼šè¯»å–PDFå’ŒWordæ–‡ä»¶
# pdf_text = read_pdf("example.pdf")
# word_text = read_word("example.docx")

# # å°†è¯»å–çš„æ–‡æœ¬æ·»åŠ åˆ°çŸ¥è¯†åº“ä¸­
# knowledge_base = [pdf_text, word_text]
# å°†è¯»å–PDFå’ŒWordæ–‡æ¡£çš„åŠŸèƒ½é›†æˆåˆ°ç°æœ‰çš„Streamlitåº”ç”¨ä¸­
# ä½ å¯ä»¥åœ¨Streamlitåº”ç”¨ä¸­æ·»åŠ æ–‡ä»¶ä¸Šä¼ åŠŸèƒ½ï¼Œå¹¶åœ¨ç”¨æˆ·ä¸Šä¼ æ–‡ä»¶åè¯»å–å…¶å†…å®¹å¹¶æ·»åŠ åˆ°çŸ¥è¯†åº“ä¸­ã€‚
