# å¯¼å…¥æ‰€éœ€çš„åº“
import streamlit as st
from modelscope import snapshot_download
from typing import List
import numpy as np
import torch
from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForSeq2Seq
import os
from datasets import load_dataset, Dataset

# è®¾ç½®CUDAç¯å¢ƒå˜é‡
os.environ["CUDA_LAUNCH_BLOCKING"] = "1"

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="ByteBrain",
    page_icon="ğŸª",
    layout="wide",
    initial_sidebar_state="auto",
    menu_items={
        "Get Help": "https://bytebrain.com/help",
        "Report a bug": "https://bytebrain.com/bug-report",
        "About": "https://bytebrain.com/about"
    }
)

# æ·»åŠ è‡ªå®šä¹‰CSSæ ·å¼
st.markdown(
    """
    <style>
    .main {
        background-image: url('Background.png');
        background-size: cover;
        background-position: center;
        padding: 10px;
    }
    .stButton>button {
        background-color: #4CAF50;
        color: blue;
        border: none;
        padding: 10px 24px;
        text-align: center;
        text-decoration: none;
        display: inline-block;
        font-size: 16px;
        margin: 4px 2px;
        cursor: pointer;
        border-radius: 12px;
    }
    .stTextInput>div>div>input {
        border: 2px solid #4CAF50;
        border-radius: 12px;
        padding: 10px;
    }
    .fixed-right {
        position: fixed;
        top: 20px;
        right: 20px;
        width: 30%;
    }
    .chat-container {
        width: 100%;
        max-height: 70vh;
        overflow-y: auto;
        padding-right: 20px;
    }
    .stChatMessage {
        width: 100%;
    }
    </style>
    """,
    unsafe_allow_html=True,
)

# åˆ›å»ºä¸€ä¸ªæ ‡é¢˜å’Œä¸€ä¸ªå‰¯æ ‡é¢˜
st.markdown(
    '<span style="font-size: 60px">âœ¨ ByteBrain</span>&nbsp;&nbsp;<span style="font-size: 24px">â€”â€”è®¡ç®—æœºç§‘å­¦æ™ºèƒ½çŸ¥è¯†åŠ©æ‰‹</span>',
    unsafe_allow_html=True
)

# å‘é‡æ¨¡å‹ä¸‹è½½
embed_model_dir = snapshot_download("AI-ModelScope/bge-small-zh-v1.5", cache_dir='.')
# æºå¤§æ¨¡å‹ä¸‹è½½
llm_model_dir = snapshot_download('IEITYuan/Yuan2-2B-Mars-hf', cache_dir='.')

# å®šä¹‰å‘é‡æ¨¡å‹ç±»
class EmbeddingModel:
    """
    class for EmbeddingModel
    """

    def __init__(self, path: str) -> None:
        # åŠ è½½é¢„è®­ç»ƒçš„åˆ†è¯å™¨å’Œæ¨¡å‹
        self.tokenizer = AutoTokenizer.from_pretrained(path)
        self.model = AutoModel.from_pretrained(path)
        if torch.cuda.is_available():
            self.model = self.model.cuda()
        print(f'Loading EmbeddingModel from {path}.')

    def get_embeddings(self, texts: List) -> List[float]:
        """
        calculate embedding for text list
        """
        # å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œç¼–ç 
        encoded_input = self.tokenizer(texts, padding=True, truncation=True, return_tensors='pt')
        if torch.cuda.is_available():
            encoded_input = {k: v.cuda() for k, v in encoded_input.items()}
        with torch.no_grad():
            # è·å–æ¨¡å‹è¾“å‡º
            model_output = self.model(**encoded_input)
            sentence_embeddings = model_output[0][:, 0]
        # å¯¹åµŒå…¥å‘é‡è¿›è¡Œå½’ä¸€åŒ–
        sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)
        return sentence_embeddings.tolist()

print("> Create embedding model...")
embed_model = EmbeddingModel(embed_model_dir)

# å®šä¹‰å‘é‡åº“ç´¢å¼•ç±»
class VectorStoreIndex:
    """
    class for VectorStoreIndex
    """

    def __init__(self, document_path: str, embed_model: EmbeddingModel) -> None:
        # åŠ è½½æ–‡æ¡£
        self.documents = []
        for line in open(document_path, 'r', encoding='utf-8'):
            line = line.strip()
            self.documents.append(line)

        self.embed_model = embed_model
        # è·å–æ–‡æ¡£çš„åµŒå…¥å‘é‡
        self.vectors = self.embed_model.get_embeddings(self.documents)

        print(f'Loading {len(self.documents)} documents for {document_path}.')

    def get_similarity(self, vector1: List[float], vector2: List[float]) -> float:
        """
        calculate cosine similarity between two vectors
        """
        dot_product = np.dot(vector1, vector2)
        magnitude = np.linalg.norm(vector1) * np.linalg.norm(vector2)
        if not magnitude:
            return 0
        return dot_product / magnitude

    def query(self, question: str, k: int = 1) -> List[str]:
        # è·å–é—®é¢˜çš„åµŒå…¥å‘é‡
        question_vector = self.embed_model.get_embeddings([question])[0]
        # è®¡ç®—é—®é¢˜å‘é‡ä¸æ–‡æ¡£å‘é‡çš„ç›¸ä¼¼åº¦
        result = np.array([self.get_similarity(question_vector, vector) for vector in self.vectors])
        # è¿”å›ç›¸ä¼¼åº¦æœ€é«˜çš„æ–‡æ¡£
        return np.array(self.documents)[result.argsort()[-k:][::-1]].tolist()

print("> Create index...")
document_path = './knowledge.txt'
index = VectorStoreIndex(document_path, embed_model)

# å®šä¹‰å¤§è¯­è¨€æ¨¡å‹ç±»
class LLM:
    """
    class for Yuan2.0 LLM
    """

    def __init__(self, model_path: str) -> None:
        print("Create tokenizer...")
        # åŠ è½½é¢„è®­ç»ƒçš„åˆ†è¯å™¨
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, add_eos_token=False, add_bos_token=False, eos_token='<eod>')
        self.tokenizer.add_tokens(['<sep>', '<pad>', '<mask>', '<predict>', '<FIM_SUFFIX>', '<FIM_PREFIX>', '<FIM_MIDDLE>','<commit_before>','<commit_msg>','<commit_after>','<jupyter_start>','<jupyter_text>','<jupyter_code>','<jupyter_output>','<empty_output>'], special_tokens=True)

        print("Create model...")
        # åŠ è½½é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹
        self.model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16, trust_remote_code=True)
        if torch.cuda.is_available():
            self.model = self.model.cuda()

        print(f'Loading Yuan2.0 model from {model_path}.')

    def generate(self, question: str, context: List):
        # æ„å»ºæç¤ºè¯
        if context:
            prompt = f'èƒŒæ™¯ï¼š{context}\né—®é¢˜ï¼š{question}\nè¯·åŸºäºèƒŒæ™¯ï¼Œå›ç­”é—®é¢˜ã€‚'
        else:
            prompt = question

        prompt += "<sep>"
        inputs = self.tokenizer(prompt, return_tensors="pt")["input_ids"]
        if torch.cuda.is_available():
            inputs = inputs.cuda()

        # æˆªæ–­è¾“å…¥ä»¥ç¡®ä¿é•¿åº¦ä¸è¶…è¿‡æœ€å¤§é•¿åº¦
        max_length = 2048  # å¢å¤§æœ€å¤§é•¿åº¦é™åˆ¶
        if inputs.shape[1] > max_length:
            inputs = inputs[:, -max_length:]

        # ç”Ÿæˆè¾“å‡º
        outputs = self.model.generate(inputs, do_sample=False, max_new_tokens=512)  # å¢å¤§æœ€å¤§ç”Ÿæˆé•¿åº¦
        output = self.tokenizer.decode(outputs[0])

        # ç§»é™¤ä¸éœ€è¦çš„å­—ç¬¦
        output = output.split("<sep>")[-1].replace("<eod>", "").strip()

        return output

print("> Create Yuan2.0 LLM...")
llm = LLM(llm_model_dir)

# å¾®è°ƒå¤§æ¨¡å‹
def fine_tune_model(model, tokenizer, train_dataset):
    # å®šä¹‰è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir='./results',
        num_train_epochs=3,
        per_device_train_batch_size=2,
        save_steps=10_000,
        save_total_limit=2,
        logging_dir='./logs',
    )

    # å®šä¹‰æ•°æ®æ•´ç†å™¨
    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

    # å®šä¹‰Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        data_collator=data_collator,
    )

    # å¼€å§‹è®­ç»ƒ
    trainer.train()

# åŠ è½½å¾®è°ƒæ•°æ®
def load_fine_tune_data(file_path):
    data = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            question, answer = line.strip().split('\t')
            data.append({'question': question, 'answer': answer})
    return Dataset.from_list(data)

# åŠ è½½å¾®è°ƒæ•°æ®
fine_tune_data_path = './fine_tune_data.txt'
fine_tune_data = load_fine_tune_data(fine_tune_data_path)

# å¾®è°ƒæ¨¡å‹
fine_tune_model(llm.model, llm.tokenizer, fine_tune_data)

# åˆæ¬¡è¿è¡Œæ—¶ï¼Œsession_stateä¸­æ²¡æœ‰"messages"ï¼Œéœ€è¦åˆ›å»ºä¸€ä¸ªç©ºåˆ—è¡¨
if "messages" not in st.session_state:
    st.session_state["messages"] = []

# ä½¿ç”¨åˆ†æ å¸ƒå±€
col1, col2 = st.columns([2, 1])

with col1:
    st.markdown("<div class='chat-container'>", unsafe_allow_html=True)
    # æ¯æ¬¡å¯¹è¯æ—¶ï¼Œéƒ½éœ€è¦éå†session_stateä¸­çš„æ‰€æœ‰æ¶ˆæ¯ï¼Œå¹¶æ˜¾ç¤ºåœ¨èŠå¤©ç•Œé¢ä¸Š
    for msg in st.session_state.messages:
        st.chat_message(msg["role"]).write(msg["content"])
    st.markdown("</div>", unsafe_allow_html=True)

with col2:
    st.markdown("<div class='fixed-right'>", unsafe_allow_html=True)
    st.image("logo.png", caption="ByteBrain Logo", width=150)
    st.markdown("ByteBrainâ€”â€”ä¸€ä¸ªæ™ºèƒ½çŸ¥è¯†åŠ©æ‰‹ï¼Œæ—¨åœ¨å¸®åŠ©ç”¨æˆ·å¿«é€Ÿè·å–ä¿¡æ¯å’Œè§£å†³é—®é¢˜ã€‚")
    st.markdown("### è”ç³»æˆ‘ä»¬")
    st.markdown("å¦‚æœæ‚¨æœ‰ä»»ä½•é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼è”ç³»æˆ‘ä»¬ï¼š")
    st.markdown("- é‚®ç®±: support@bytebrain.com")
    st.markdown("- ç”µè¯: 520-1314")
    st.markdown("</div>", unsafe_allow_html=True)

# èŠå¤©è¾“å…¥æ¡†æ”¾åœ¨col1ä¹‹å¤–
prompt = st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜:")

if prompt:
    # å°†ç”¨æˆ·çš„è¾“å…¥æ·»åŠ åˆ°session_stateä¸­çš„messagesåˆ—è¡¨ä¸­
    st.session_state.messages.append({"role": "user", "content": prompt})

    # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºç”¨æˆ·çš„è¾“å…¥
    st.chat_message("user").write(prompt)

    # æ£€ç´¢ç›¸å…³çŸ¥è¯†
    context = index.query(prompt)

    # è°ƒç”¨æ¨¡å‹
    response = llm.generate(prompt, context)

    # å°†æ¨¡å‹çš„è¾“å‡ºæ·»åŠ åˆ°session_stateä¸­çš„messagesåˆ—è¡¨ä¸­
    st.session_state.messages.append({"role": "assistant", "content": response})

    # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºæ¨¡å‹çš„è¾“å‡º
    st.chat_message("assistant").write(response)
